{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Assignment\n",
    "Author: VÃ­ctor Lavado Campos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of observations and its proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 1000\n",
    "p = 20\n",
    "\n",
    "alpha_1 = np.random.uniform(low=0.2, high=0.3, size=None)\n",
    "alpha_2 = np.random.uniform(low=0.2, high=0.3, size=None)\n",
    "\n",
    "n1 = round(alpha_1*n)\n",
    "n2 = round(alpha_2*n)\n",
    "n3 = n - (n1 + n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random vectors and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two random vector from a standard normal distribution\n",
    "u1_vector = np.random.normal(loc=0.0, scale=1.0, size=p)\n",
    "u2_vector = np.random.normal(loc=0.0, scale=1.0, size=p)\n",
    "\n",
    "# Unit norm vector\n",
    "u1 = u1_vector/(np.linalg.norm(u1_vector, ord=None, axis=None, keepdims=False))\n",
    "u2 = u2_vector/(np.linalg.norm(u2_vector, ord=None, axis=None, keepdims=False))\n",
    "\n",
    "# Generate constants\n",
    "gamma_1 = 5\n",
    "gamma_2 = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of u1 is 0.9999999999999999\n",
      "The norm of u2 is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check that u1 and u2 are unit norm vectors\n",
    "print(\"The norm of u1 is {}\".format(np.linalg.norm(u1, ord=None, axis=None, keepdims=False)))\n",
    "print(\"The norm of u2 is {}\".format(np.linalg.norm(u2, ord=None, axis=None, keepdims=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random diagonal matrices with values from uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random diagonal matrixes \n",
    "diag_1 = np.random.uniform(low=0.25, high=4, size=20)\n",
    "D1 = np.diag(diag_1)\n",
    "\n",
    "diag_2 = np.random.uniform(low=0.25, high=4, size=20)\n",
    "D2 = np.diag(diag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "(20, 20)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of the matrixes\n",
    "print(D1.shape)\n",
    "print(D2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $X_{1}$ and $X_{2}$ matrices from multivariate normal distribution observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of variables for each observation is 20\n",
      "The number of observations for the first group is 274\n",
      "The number of rows for matrix X1 is 274\n",
      "The number of columns for matrix X1 is 20\n",
      "The number of observations for the second group is 259\n",
      "The number of rows for matrix X2 is 259\n",
      "The number of columns for X2 maxtris is 20\n"
     ]
    }
   ],
   "source": [
    "# Generate X1 and X2 matrices\n",
    "\n",
    "# Define means and covariances matrixes for i=1,2\n",
    "mean1 = u1*gamma_1\n",
    "cov1 = D1\n",
    "mean2 = u2*gamma_2\n",
    "cov1 = D2\n",
    "\n",
    "n1_obs = np.random.multivariate_normal(mean1, cov1, size=(n1))\n",
    "X1 = n1_obs\n",
    "\n",
    "n2_obs = np.random.multivariate_normal(mean1, cov1, size=(n2))\n",
    "X2 = n2_obs\n",
    "\n",
    "#Check dimensions of X1 and X2 matrixes\n",
    "\n",
    "print(\"The number of variables for each observation is {}\".format(p))\n",
    "\n",
    "print(\"The number of observations for the first group is {}\".format(n1))\n",
    "print(\"The number of rows for matrix X1 is {}\".format(X1.shape[0]))\n",
    "print(\"The number of columns for matrix X1 is {}\".format(X1.shape[1]))\n",
    "\n",
    "print(\"The number of observations for the second group is {}\".format(n2))\n",
    "print(\"The number of rows for matrix X2 is {}\".format(X2.shape[0]))\n",
    "print(\"The number of columns for X2 maxtris is {}\".format(X2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $Y_{1}$ and $Y_{2}$ as vectors of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Y1 and Y2 matrices\n",
    "\n",
    "Y1 = np.ones(n1)\n",
    "Y2 = np.ones(n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $X_{3}$ matrix from multivariate normal distribution observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of variables for each observation is 20\n",
      "The number of observations for the third group is 467\n",
      "The number of rows for matrix X3 is 467\n",
      "The number of columns for matrix X3 is 20\n"
     ]
    }
   ],
   "source": [
    "# Generate X3 matrix\n",
    "\n",
    "# Define mean and covariances matrixe for i=3\n",
    "\n",
    "mean3 = np.zeros(shape=(p))\n",
    "cov3 =  np.identity(p)\n",
    "\n",
    "n3_obs = np.random.multivariate_normal(mean3, cov3, size=(n3))\n",
    "X3 = n3_obs\n",
    "\n",
    "#Check dimensions of X3 matrix\n",
    "\n",
    "print(\"The number of variables for each observation is {}\".format(p))\n",
    "\n",
    "print(\"The number of observations for the third group is {}\".format(n3))\n",
    "print(\"The number of rows for matrix X3 is {}\".format(X3.shape[0]))\n",
    "print(\"The number of columns for matrix X3 is {}\".format(X3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $Y_{3}$ as a vector of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Y3 Matrix\n",
    "\n",
    "Y3 =np.zeros(n3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain $X$ and $Y$ matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of X matrix are (1000, 21)\n",
      "The dimensions of Y matrix are (1000,)\n"
     ]
    }
   ],
   "source": [
    "#Generate X and Y matrices\n",
    "\n",
    "X_aux = np.concatenate((X1,X2,X3), axis=0)\n",
    "Xones = np.ones(n).T\n",
    "\n",
    "X = np.concatenate((Xones[:,None], X_aux), axis=1)\n",
    "Y = np.concatenate((Y1.T, Y2.T, Y3.T), axis=0)\n",
    "\n",
    "#Check the dimensions of X and Y\n",
    "\n",
    "print(\"The dimensions of X matrix are {}\".format(X.shape))\n",
    "print(\"The dimensions of Y matrix are {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data using $u_{1}$ as one of the axis. PCA (Principal Component Analysis) has been applied to reduce the dimensionality from $p=20$ to $p=2$ just in this section to ease the graphical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(1000, 20)\n",
      "(1001, 20)\n"
     ]
    }
   ],
   "source": [
    "# Plot the data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(u1[:,None].T.shape)\n",
    "print(X_aux.shape)\n",
    "# Concatenate u1 with X_aux (same as X with out the column of ones)\n",
    "Mat_x = np.concatenate((u1[:,None].T, X_aux),axis=0)\n",
    "print(Mat_x.shape)\n",
    "\n",
    "# Reduce from p dimensions to 2 dimensions by performing PCA \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_plot = pca.fit_transform(Mat_x)\n",
    "\n",
    "#The first axis is obtained by performing PCA on u1\n",
    "\n",
    "Ax1 = X_plot[0,:]\n",
    "\n",
    "# The second axis is obtained manually, by making sure it is perpendicular to Ax1\n",
    "\n",
    "Ax2 = np.array([Ax1[1],-Ax1[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check they are perpendicualar by doing dot product\n",
    "# The output should be zero\n",
    "\n",
    "np.dot(Ax1, Ax2, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first row of X_plot is not really an observation but u1\n",
    "# Let's get rid of it\n",
    "\n",
    "X_plot = np.delete(X_plot, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ae323bbd0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the coordinates of the observations after transformed by PCA\n",
    "\n",
    "# X coords are the dot products of the observation with Ax1\n",
    "X_coord = np.array([])\n",
    "# Y coords are the dot products of the observation with Ax2\n",
    "Y_coord = np.array([])\n",
    "\n",
    "for i in range(0, X_plot.shape[0]):\n",
    "    X_values = np.dot(Ax1, X_plot[i,:]).tolist()\n",
    "    Y_values = np.dot(Ax2, X_plot[i,:]).tolist()\n",
    "    X_coord = np.append(X_coord,X_values)\n",
    "    Y_coord = np.append(Y_coord,Y_values)\n",
    "    \n",
    "# Separate the coordinates accoding to the three samples\n",
    "X1_cord = X_coord[0:n1]\n",
    "X2_cord = X_coord[n1+1:n1+n2]\n",
    "X3_cord = X_coord[n1+n2+1:n]\n",
    "\n",
    "Y1_cord = Y_coord[0:n1]\n",
    "Y2_cord = Y_coord[n1+1:n1+n2]\n",
    "Y3_cord = Y_coord[n1+n2+1:n]\n",
    "\n",
    "#plot the data\n",
    "\n",
    "plt.plot(X1_cord, Y1_cord, 'ro')\n",
    "plt.plot(X2_cord, Y2_cord, 'bo')\n",
    "plt.plot(X3_cord, Y3_cord, 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations of the third group (in green) are clearly separated from the rest.\n",
    "The observations of the first group (in red) and the observations of the second group (in blue) are more overlaped. However, it is possible to appreciate the differences of the first and second group due to the different means and variance-covariance matrixes used to generate the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribute the observations into training and testing partitions according to their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the observations into training and testing set\n",
    "# 80% training and 20% testing by selecting the index randomly\n",
    "import random as rd\n",
    "\n",
    "# Generate the random index list for testing\n",
    "Is = rd.sample(range(0,1000), k = int(0.2*n))\n",
    "\n",
    "# Generate the index list for training\n",
    "It =[]\n",
    "\n",
    "for i in range(0,1000):\n",
    "    if i in Is:\n",
    "        pass\n",
    "    else: \n",
    "        It.append(i)\n",
    "\n",
    "# Insert the observations into Xs, Ys, Xt and Yt according to the index\n",
    "\n",
    "Xs = np.empty((0,p+1))\n",
    "Ys = np.empty((0,1))\n",
    "Xt = np.empty((0,p+1))\n",
    "Yt = np.empty((0,1))\n",
    "\n",
    "for i in range(0,1000):\n",
    "    if i in Is:\n",
    "        Xs = np.vstack((Xs,X[i]))\n",
    "        Ys = np.vstack((Ys,Y[i]))\n",
    "    else:\n",
    "        Xt = np.vstack((Xt,X[i]))\n",
    "        Yt = np.vstack((Yt,Y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that training and testing split has been done correctly, the dimensions of the matrices will be printed.<br>\n",
    "**Xs**: should have 20% of the observations (as rows) and p+1 variables (as columns)<br>\n",
    "**Ys**: should have 20% of the responses (as rows) and 1 column<br>\n",
    "**Xt**: should have 80% of the observations (as rows) and p+1 variables (as columns)<br>\n",
    "**Yt**: should have 80% of the responses (as rows) and 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Xs the number of rows is 200 and columns is 21\n",
      "In Ys the number of rows is 200\n",
      "In Xt the number of rows is 800 and columns is 21\n",
      "In Yt the number of rows is 800\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions of Xs, Ys, Xt and Xt matrixes.\n",
    "print(\"In Xs the number of rows is {} and columns is {}\".format(Xs.shape[0],Xs.shape[1]))\n",
    "print(\"In Ys the number of rows is {}\".format(Ys.shape[0]))\n",
    "print(\"In Xt the number of rows is {} and columns is {}\".format(Xt.shape[0],Xt.shape[1]))\n",
    "print(\"In Yt the number of rows is {}\".format(Yt.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least-squares with Scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section least squares criterion is implement as the objective function. Then *scipi.optimize* library is called through *minimize* module to optimize the objective function by finding the minimum least square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.optimize import minimize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "\n",
      "Values of the least squares coefficients obtained with Nelder-Mead:\n",
      "beta   0   0.263\n",
      "beta   1  -0.515\n",
      "beta   2   0.348\n",
      "beta   3  -0.764\n",
      "beta   4   0.124\n",
      "beta   5  -0.611\n",
      "beta   6  -0.049\n",
      "beta   7  -0.224\n",
      "beta   8  -0.350\n",
      "beta   9   0.134\n",
      "beta  10  -0.617\n",
      "beta  11   0.518\n",
      "beta  12   0.201\n",
      "beta  13  -0.361\n",
      "beta  14   0.470\n",
      "beta  15   0.293\n",
      "beta  16   0.827\n",
      "beta  17  -0.112\n",
      "beta  18  -0.070\n",
      "beta  19  -0.494\n",
      "beta  20   0.263\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function as the sum of squares of the errors\n",
    "\n",
    "def least_sq(beta, X, Y):\n",
    "    error = np.empty((0, p+1))\n",
    "    for i in range (0,X.shape[0]):\n",
    "        exp = np.exp(-beta*X[i])\n",
    "        value = (Y[i]-1/(1+exp))**2\n",
    "        error = np.append(error, np.array([value]),axis=0)\n",
    "    error = np.sum(error)\n",
    "    return error\n",
    "\n",
    "# Initialize the values of beta\n",
    "beta0 = np.zeros(p+1)\n",
    "\n",
    "# Minimize the objective function using \"Nelder-Mead\"\n",
    "# Arguments are from the training partition\n",
    "result = minimize(least_sq, beta0, args=(Xt, Yt), method='Nelder-Mead', options={'disp': True,'xtol': 1e-10})\n",
    "\n",
    "## Print results\n",
    "\n",
    "print('\\nValues of the least squares coefficients obtained with Nelder-Mead:')\n",
    "for i in range(p+1):\n",
    "    print('beta %3d %7.3f' %(i,result.x[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *minimize* function documentations allows a number of solvers to be used. In this case *Melder-Mead* is applied because it is a free-derivative method, and therefore more straight forward. On top of that, the statement of the section does not ask specifically por any particular method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the results obtained, an important finding is that beta0 (sometimes called intercept) and the rest of the betas (sometimes called slopes), they almost have all the values concentrated in beteween -1 and 1<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty I found in my case was to deal with the definition of the function *least_sq*. At first I tried to implement the function using just *numpy* library, but that proved to be too complex because the outputs of the intermediate operations was not clear.<br>\n",
    "\n",
    "To solve the issue, I applied an element wise approach using loops and then aggregating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least-squares with Bounds on the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3 the goal is to minimize the least squares objective function by imposing bounds on the $\\beta$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that purpose, firstly, the objective function of section 2 is designed. Secondly, the gradient of the objective function is computed. \n",
    "As there are $p+1$ $\\beta$ parameters to be estimated, the output of the gradient should be a $p+1$ dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import time\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is the same\n",
    "\n",
    "def least_sq(beta, X, Y):\n",
    "    error = np.empty((0, p+1))\n",
    "    for i in range (0,X.shape[0]):\n",
    "        exp = np.exp(-beta*X[i])\n",
    "        value = (Y[i]-1/(1+exp))**2\n",
    "        error = np.append(error, np.array([value]),axis=0)\n",
    "    error = np.sum(error)\n",
    "    return error\n",
    "\n",
    "# Initialize the values of beta\n",
    "beta0 = np.zeros(p+1)\n",
    "\n",
    "# Define the gradient (jacobian) of the objective function\n",
    "def grad_least_sq(beta, X, Y):\n",
    "    beta = np.array(beta)\n",
    "    beta = beta.flatten()\n",
    "    gg = np.empty((X.shape[0],beta.shape[0]))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        for j in range(0, beta.shape[0]):\n",
    "            comp = -2*(X[i,j]*math.exp(-beta[j]*X[i,j])*(Y[i]-1/(1+math.exp(-beta[j]*X[i,j]))))/(1+math.exp(-beta[j]*X[i,j]))**2\n",
    "            gg[i,j] = comp\n",
    "    gg = np.sum(gg, axis=0)\n",
    "    return gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference approx:    30.369359\n",
      "Directional derivative:      30.370916\n"
     ]
    }
   ],
   "source": [
    "# Check that the derivative routine for the objective function is computing correct values\n",
    "\n",
    "## Initial values for  beta, u and delta\n",
    "\n",
    "beta_0 = np.ones(p+1)\n",
    "delta_v = 1.0e-4\n",
    "u = np.random.normal(0,1,size=p+1)\n",
    "u = u/np.linalg.norm(u,ord=2)\n",
    "\n",
    "## Values of the objective function at  alpha  and  alpha + delta*u\n",
    "\n",
    "beta_1 = beta_0 + delta_v*u\n",
    "\n",
    "v1 = least_sq(beta_1,X,Y)\n",
    "v0 = least_sq(beta_0,X,Y)\n",
    "\n",
    "## Compute the gradient at  alpha\n",
    "\n",
    "grad_0 = grad_least_sq(beta_0,X,Y)\n",
    "\n",
    "## Compare the value of the directional derivative  g'u  with the \n",
    "## finite difference approximation  (f_1 - f_0)/delta\n",
    "\n",
    "fd_appr = (v1 - v0)/delta_v\n",
    "dir_der = np.dot(grad_0.T,u)\n",
    "print('Finite difference approx: %12.6f' %fd_appr)\n",
    "print('Directional derivative:   %12.6f' %dir_der)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both metods output similar values, therefore the derivative routine for the objective function is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three algorithms will be applied to compute the optimization model: **SLSQP, TNC, Melder-Mead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 3729.2963371874184\n",
      "            Iterations: 30\n",
      "            Function evaluations: 69\n",
      "            Gradient evaluations: 30\n",
      "Running time = 10.63653\n",
      "     fun: 3729.2963371874184\n",
      "     jac: array([-2.56922940e-05,  2.63790540e-04,  3.22842347e-04,  2.85509760e-04,\n",
      "       -1.01117063e-03,  2.46823477e-04,  1.15171343e-03,  1.25905054e-04,\n",
      "       -4.94703798e-05, -5.96256509e-04,  4.18880749e-04,  6.62297860e-04,\n",
      "       -3.63111890e-04, -2.30356206e-04, -1.15797101e-03, -5.67107718e-04,\n",
      "       -6.03374725e-04, -1.20447425e-03, -4.42561754e-05,  1.46487975e-03,\n",
      "       -1.05991149e-03])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 69\n",
      "     nit: 30\n",
      "    njev: 30\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.09507129, -0.80120946,  0.58499143, -0.9493628 ,  0.26586757,\n",
      "       -0.78764014, -0.14701094, -0.08257897, -0.30590452, -0.13888823,\n",
      "       -0.71510712,  0.57256934,  0.06820257, -0.79907733,  0.25594041,\n",
      "        0.37094843,  0.79823014,  0.19109672, -0.55641593, -0.2437502 ,\n",
      "        0.26392278])\n",
      "\n",
      "Values of the least squares coefficients obtained with SLSQP:\n",
      "beta   0   0.095\n",
      "beta   1  -0.801\n",
      "beta   2   0.585\n",
      "beta   3  -0.949\n",
      "beta   4   0.266\n",
      "beta   5  -0.788\n",
      "beta   6  -0.147\n",
      "beta   7  -0.083\n",
      "beta   8  -0.306\n",
      "beta   9  -0.139\n",
      "beta  10  -0.715\n",
      "beta  11   0.573\n",
      "beta  12   0.068\n",
      "beta  13  -0.799\n",
      "beta  14   0.256\n",
      "beta  15   0.371\n",
      "beta  16   0.798\n",
      "beta  17   0.191\n",
      "beta  18  -0.556\n",
      "beta  19  -0.244\n",
      "beta  20   0.264\n"
     ]
    }
   ],
   "source": [
    "# Let's implement two different procedures with bounds on the variables\n",
    "\n",
    "# The result from the previous section showed that betas were in between almost in [-1, 1] interval, \n",
    "# so that interval will be considered as the bounds\n",
    "\n",
    "lower = -np.ones(p+1)\n",
    "upper = np.ones(p+1)\n",
    "\n",
    "# Define bounds\n",
    "\n",
    "bnds = Bounds(lower, upper)\n",
    "\n",
    "# Initialize the values of beta\n",
    "\n",
    "beta0 = np.zeros(p+1)\n",
    "\n",
    "# First procedure SLSQP\n",
    "\n",
    "time_start1 = time.process_time()\n",
    "result1 = minimize(least_sq,beta0,args=(Xt,Yt),method='SLSQP',jac=grad_least_sq,bounds=bnds,options={'disp': True})\n",
    "time_elapsed1 = (time.process_time() - time_start1) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed1))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result1)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the least squares coefficients obtained with SLSQP:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result1.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time = 13.73651\n",
      "     fun: 3729.2963371493815\n",
      "     jac: array([-2.08748161e-05,  8.86030036e-06, -2.29002445e-06, -1.46363193e-05,\n",
      "       -7.52247409e-05,  2.92006631e-05,  3.32865867e-06, -8.51617287e-05,\n",
      "        2.31149704e-05, -8.79403513e-07, -1.93424513e-06,  6.15852156e-05,\n",
      "       -7.79745103e-05, -7.61645944e-05, -2.49399513e-05, -1.63220840e-05,\n",
      "       -1.48209053e-05,  3.91507688e-05,  7.90369160e-06, -1.82031076e-04,\n",
      "       -3.31461059e-05])\n",
      " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
      "    nfev: 42\n",
      "     nit: 10\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 0.09507134, -0.80121384,  0.58498806, -0.94936914,  0.2658734 ,\n",
      "       -0.78764312, -0.147024  , -0.08258008, -0.30590415, -0.13888517,\n",
      "       -0.71511221,  0.57256367,  0.06820415, -0.79907529,  0.25594653,\n",
      "        0.37095182,  0.79823729,  0.19110582, -0.55641532, -0.24375744,\n",
      "        0.26392906])\n",
      "\n",
      "Values of the least squares coefficients obtained with TNC:\n",
      "beta   0   0.095\n",
      "beta   1  -0.801\n",
      "beta   2   0.585\n",
      "beta   3  -0.949\n",
      "beta   4   0.266\n",
      "beta   5  -0.788\n",
      "beta   6  -0.147\n",
      "beta   7  -0.083\n",
      "beta   8  -0.306\n",
      "beta   9  -0.139\n",
      "beta  10  -0.715\n",
      "beta  11   0.573\n",
      "beta  12   0.068\n",
      "beta  13  -0.799\n",
      "beta  14   0.256\n",
      "beta  15   0.371\n",
      "beta  16   0.798\n",
      "beta  17   0.191\n",
      "beta  18  -0.556\n",
      "beta  19  -0.244\n",
      "beta  20   0.264\n"
     ]
    }
   ],
   "source": [
    "# Second procedure using TNC\n",
    "\n",
    "time_start2 = time.process_time()\n",
    "result2 = minimize(least_sq,beta0,args=(Xt,Yt),method='TNC',jac=grad_least_sq,bounds=bnds,options={'disp': True})\n",
    "time_elapsed2 = (time.process_time() - time_start2) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Elapsed time = %8.5f' %(time_elapsed2))\n",
    "\n",
    "# Print the output\n",
    "\n",
    "print(result2)\n",
    "\n",
    "# Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the least squares coefficients obtained with TNC:')\n",
    "for i in range(p+1):\n",
    "    print('beta %3d %7.3f' %(i,result2.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Elapsed time = 122.52428\n",
      " final_simplex: (array([[ 0.26342056, -0.51476689,  0.34795249, -0.76430795,  0.12397376,\n",
      "        -0.61059554, -0.04940258, -0.22365583, -0.34970187,  0.1335899 ,\n",
      "        -0.616851  ,  0.51804681,  0.20077316, -0.361297  ,  0.47040944,\n",
      "         0.29253097,  0.82673951, -0.11249968, -0.07037682, -0.49368342,\n",
      "         0.26305985],\n",
      "       [ 0.26752444, -0.52511327,  0.35574236, -0.75446137,  0.1095816 ,\n",
      "        -0.6086754 , -0.04442195, -0.22282422, -0.34102885,  0.1296371 ,\n",
      "        -0.62406911,  0.5201696 ,  0.20814228, -0.36747037,  0.47047725,\n",
      "         0.29790366,  0.82825957, -0.10374235, -0.06949744, -0.51461702,\n",
      "         0.26263058],\n",
      "       [ 0.26561254, -0.52597768,  0.35869407, -0.75914894,  0.11681438,\n",
      "        -0.61480226, -0.04184431, -0.22419542, -0.35266994,  0.13103484,\n",
      "        -0.62895735,  0.52223741,  0.20616338, -0.36815327,  0.47576273,\n",
      "         0.29802764,  0.83519287, -0.10699344, -0.06737723, -0.51450486,\n",
      "         0.26460784],\n",
      "       [ 0.26647318, -0.52920558,  0.35816457, -0.75387469,  0.11733926,\n",
      "        -0.60487447, -0.04050099, -0.22523925, -0.35997288,  0.13321288,\n",
      "        -0.63180609,  0.51900302,  0.20566397, -0.36581973,  0.47760851,\n",
      "         0.2962654 ,  0.83516307, -0.11227255, -0.07202595, -0.50128499,\n",
      "         0.26218711],\n",
      "       [ 0.2650017 , -0.51966866,  0.36478498, -0.75172676,  0.1103508 ,\n",
      "        -0.61270772, -0.04704081, -0.22480731, -0.34931027,  0.12520981,\n",
      "        -0.62451061,  0.51754455,  0.20306451, -0.36390419,  0.47375453,\n",
      "         0.2956301 ,  0.83068337, -0.10768754, -0.06434833, -0.50832552,\n",
      "         0.26046588],\n",
      "       [ 0.26309852, -0.51940724,  0.36386348, -0.7593117 ,  0.12376858,\n",
      "        -0.60834366, -0.05312346, -0.2290047 , -0.35487029,  0.12572608,\n",
      "        -0.62531121,  0.5154442 ,  0.20226843, -0.36230085,  0.47559455,\n",
      "         0.2917869 ,  0.83427689, -0.11435186, -0.06299503, -0.50134207,\n",
      "         0.26561626],\n",
      "       [ 0.27254025, -0.51487124,  0.36433138, -0.75957743,  0.12103286,\n",
      "        -0.62440705, -0.05372286, -0.22993041, -0.35389227,  0.12646306,\n",
      "        -0.62945783,  0.51187969,  0.19820967, -0.36339482,  0.47863304,\n",
      "         0.30037115,  0.83336649, -0.10679501, -0.05877225, -0.50624473,\n",
      "         0.25416963],\n",
      "       [ 0.2643482 , -0.50030407,  0.36435837, -0.76494832,  0.12898519,\n",
      "        -0.61075657, -0.05403411, -0.225675  , -0.3614974 ,  0.11997654,\n",
      "        -0.61964664,  0.51261465,  0.19170951, -0.36324556,  0.47398485,\n",
      "         0.29453198,  0.82580847, -0.10582757, -0.05165876, -0.50681016,\n",
      "         0.25174692],\n",
      "       [ 0.26532058, -0.52314667,  0.36198866, -0.74502609,  0.10998218,\n",
      "        -0.62657947, -0.04465414, -0.22611122, -0.34853682,  0.13063028,\n",
      "        -0.62665257,  0.51372147,  0.20275661, -0.35923764,  0.47594128,\n",
      "         0.29645276,  0.83255865, -0.1106076 , -0.06712955, -0.50051664,\n",
      "         0.26167602],\n",
      "       [ 0.2670918 , -0.52095579,  0.35508089, -0.74836935,  0.1171501 ,\n",
      "        -0.61995136, -0.05168457, -0.22836254, -0.34290811,  0.13174274,\n",
      "        -0.62304619,  0.50867566,  0.20283074, -0.35713909,  0.47279816,\n",
      "         0.29448242,  0.82826275, -0.11207368, -0.06782723, -0.49387458,\n",
      "         0.26231314],\n",
      "       [ 0.27841472, -0.52631697,  0.3609867 , -0.75195052,  0.12420651,\n",
      "        -0.62127572, -0.05073572, -0.23342963, -0.3551695 ,  0.13155744,\n",
      "        -0.639624  ,  0.50531153,  0.20229631, -0.36384175,  0.48197414,\n",
      "         0.30361714,  0.83567796, -0.10685803, -0.06203285, -0.50518646,\n",
      "         0.25357046],\n",
      "       [ 0.26478877, -0.5290955 ,  0.35468979, -0.74711415,  0.12003407,\n",
      "        -0.62301998, -0.03505131, -0.22400026, -0.35248574,  0.1346031 ,\n",
      "        -0.63231372,  0.51447544,  0.20606907, -0.36516445,  0.47623934,\n",
      "         0.29946101,  0.83291221, -0.10437515, -0.06532426, -0.51671041,\n",
      "         0.26418292],\n",
      "       [ 0.2632578 , -0.52600977,  0.3590602 , -0.74508309,  0.12125355,\n",
      "        -0.61801254, -0.04268548, -0.22743196, -0.35450444,  0.13171371,\n",
      "        -0.62990305,  0.51023367,  0.20360962, -0.35993015,  0.47665586,\n",
      "         0.29456758,  0.83301589, -0.11176641, -0.06516471, -0.50213188,\n",
      "         0.26468368],\n",
      "       [ 0.26300018, -0.51735854,  0.36043526, -0.74849872,  0.12493982,\n",
      "        -0.61592547, -0.04936213, -0.22809189, -0.350641  ,  0.12633759,\n",
      "        -0.6249942 ,  0.50721444,  0.20071184, -0.35943092,  0.47374111,\n",
      "         0.29344113,  0.82851064, -0.10894604, -0.05845196, -0.50572178,\n",
      "         0.26246215],\n",
      "       [ 0.26466544, -0.52571182,  0.35533913, -0.75597736,  0.13482077,\n",
      "        -0.62729483, -0.046399  , -0.23072352, -0.35427708,  0.13372916,\n",
      "        -0.63251279,  0.50944482,  0.20366903, -0.36181764,  0.4785798 ,\n",
      "         0.29644266,  0.8370278 , -0.11049638, -0.06031196, -0.50990565,\n",
      "         0.26886546],\n",
      "       [ 0.27145627, -0.51952359,  0.358717  , -0.74489851,  0.12417472,\n",
      "        -0.62297739, -0.04390229, -0.22863769, -0.3583025 ,  0.13107128,\n",
      "        -0.63363783,  0.50361628,  0.19822198, -0.36059994,  0.47897855,\n",
      "         0.30080407,  0.82956006, -0.10544164, -0.05955624, -0.50377695,\n",
      "         0.25191424],\n",
      "       [ 0.25937889, -0.51863496,  0.35745971, -0.742549  ,  0.12359821,\n",
      "        -0.6157787 , -0.03623259, -0.22294465, -0.36090129,  0.13030631,\n",
      "        -0.62663006,  0.50946802,  0.19912559, -0.36000556,  0.47489656,\n",
      "         0.29408564,  0.82749769, -0.10789269, -0.06108414, -0.50527769,\n",
      "         0.25980743],\n",
      "       [ 0.26682423, -0.52776194,  0.36047664, -0.74403678,  0.1264812 ,\n",
      "        -0.61948332, -0.03963871, -0.22868668, -0.35955071,  0.13131758,\n",
      "        -0.63718615,  0.50694596,  0.20325774, -0.36309679,  0.47978627,\n",
      "         0.29883915,  0.83444031, -0.10679318, -0.05998318, -0.51228373,\n",
      "         0.26090948],\n",
      "       [ 0.26122268, -0.51650722,  0.36499335, -0.74674824,  0.12874732,\n",
      "        -0.62591265, -0.05071927, -0.23113114, -0.35818626,  0.12694119,\n",
      "        -0.62761435,  0.50451058,  0.19735123, -0.3551021 ,  0.47835541,\n",
      "         0.29195108,  0.83356145, -0.11488725, -0.0565989 , -0.49683309,\n",
      "         0.26354662],\n",
      "       [ 0.26652418, -0.52983101,  0.35244965, -0.74271826,  0.12735268,\n",
      "        -0.63607573, -0.03786318, -0.22865718, -0.35422627,  0.13882294,\n",
      "        -0.63556906,  0.50579474,  0.20357218, -0.35898878,  0.47969733,\n",
      "         0.29960911,  0.83504705, -0.1088478 , -0.06439975, -0.50556872,\n",
      "         0.26430668],\n",
      "       [ 0.26570274, -0.51785409,  0.36006239, -0.74655455,  0.13384508,\n",
      "        -0.62671117, -0.05208963, -0.23300048, -0.35529215,  0.12923665,\n",
      "        -0.6302111 ,  0.4994347 ,  0.19794112, -0.35538846,  0.47830982,\n",
      "         0.29460898,  0.83174837, -0.11214182, -0.05568298, -0.49853165,\n",
      "         0.2613994 ],\n",
      "       [ 0.26170527, -0.51427946,  0.36424501, -0.74561172,  0.13130988,\n",
      "        -0.62433436, -0.04701657, -0.22925912, -0.35845951,  0.12492452,\n",
      "        -0.62868196,  0.5029376 ,  0.19680701, -0.35825318,  0.47716465,\n",
      "         0.29446277,  0.83021192, -0.10770612, -0.05118373, -0.50944306,\n",
      "         0.26027874]]), array([3793.68832254, 3793.70818602, 3793.71491229, 3793.76905937,\n",
      "       3793.81425556, 3793.86678561, 3793.89081364, 3794.00819555,\n",
      "       3794.01498857, 3794.06025038, 3794.14558044, 3794.21603935,\n",
      "       3794.24381536, 3794.31206808, 3794.35447367, 3794.39104778,\n",
      "       3794.42268146, 3794.46783087, 3794.50220487, 3794.57412335,\n",
      "       3794.65100611, 3794.67190639]))\n",
      "           fun: 3793.6883225367656\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 4200\n",
      "           nit: 3460\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([ 0.26342056, -0.51476689,  0.34795249, -0.76430795,  0.12397376,\n",
      "       -0.61059554, -0.04940258, -0.22365583, -0.34970187,  0.1335899 ,\n",
      "       -0.616851  ,  0.51804681,  0.20077316, -0.361297  ,  0.47040944,\n",
      "        0.29253097,  0.82673951, -0.11249968, -0.07037682, -0.49368342,\n",
      "        0.26305985])\n",
      "\n",
      "Values of the least squares coefficients obtained with Melder-Mead:\n",
      "beta   0   0.263\n",
      "beta   1  -0.515\n",
      "beta   2   0.348\n",
      "beta   3  -0.764\n",
      "beta   4   0.124\n",
      "beta   5  -0.611\n",
      "beta   6  -0.049\n",
      "beta   7  -0.224\n",
      "beta   8  -0.350\n",
      "beta   9   0.134\n",
      "beta  10  -0.617\n",
      "beta  11   0.518\n",
      "beta  12   0.201\n",
      "beta  13  -0.361\n",
      "beta  14   0.470\n",
      "beta  15   0.293\n",
      "beta  16   0.827\n",
      "beta  17  -0.112\n",
      "beta  18  -0.070\n",
      "beta  19  -0.494\n",
      "beta  20   0.263\n"
     ]
    }
   ],
   "source": [
    "# Third procedure using Nelder Mead as in the previous section\n",
    "# This method does not allow to input bounds, but it is interesting for comparing the outputs\n",
    "\n",
    "time_start3 = time.process_time()\n",
    "result3 = minimize(least_sq,beta0,args=(Xt,Yt),method='Nelder-Mead',options={'disp': True})\n",
    "time_elapsed3 = (time.process_time() - time_start3) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Elapsed time = %8.5f' %(time_elapsed3))\n",
    "\n",
    "# Print the output\n",
    "\n",
    "print(result3)\n",
    "\n",
    "# Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the least squares coefficients obtained with Melder-Mead:')\n",
    "for i in range(p+1):\n",
    "    print('beta %3d %7.3f' %(i,result3.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is table summarizing the results obtained for each of the three algorithm **SLSQP, TNC and Melder-Mead**\n",
    "\n",
    "|Method |Running time (s)|Number of iterations|Number of evaluations|OF Value|Bounds|\n",
    "|---|---|---|---|---|---|\n",
    "|**SLSQP** |\t10.63 |\t30 |\t69 |\t3729.29 |\tYes|\n",
    "|**TNC**\t|13.73\t|\t10 |\t42|\t3729.29|\tYes|\n",
    "|**Nelder Mead**\t|122.52\t|\t3460|\t4200|\t3793.68 |\tNo |\n",
    "\n",
    "**Note: the results depend on the randomness introduced when generaring the data, so they values will change if the code is executed again**\n",
    "\n",
    "The running time of Nelder Mead is very large compared to SLSQP and TNC, it might be because the the two last ones use the gradient for computing the least squares function as opossed to Melder Mead. In the same line, Melder Mead requires a lot more of iterations, in fact the function stopped after reaching the maximum number of iterations allowed, because it was not able to converge before.\n",
    "\n",
    "Now regarding the values obtained for the beta coefficients, they all are within the [-1,1].\n",
    "\n",
    "Probably the reason why the beta coefficients are contained so well in the interval has to do with the artificial data set generated that follows a multivariate gaussian distribution. In the case of data from the real world, the coefficients would probably have a bigger variance.\n",
    "\n",
    "It is also very interesing to notice that $\\beta$ estimations performed with SLSQP and TNC are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification of Observations in the Test Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first obtain the approximations of $y_{i}$ for the beta coefficients estimated by each of the three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of betas obtained by SLSQP\n",
    "beta_s = np.empty(0)\n",
    "\n",
    "# Array of betas obtained by TNC\n",
    "beta_t = np.empty(0)\n",
    "\n",
    "# Array of betas obtained by Melder-Mead\n",
    "beta_m = np.empty(0)\n",
    "\n",
    "for i in range(p+1):\n",
    "    beta_s = np.append(beta_s,result1.x[i])\n",
    "    beta_t = np.append(beta_t,result2.x[i])\n",
    "    beta_m = np.append(beta_m,result3.x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of beta of SLSQP [ 0.09507129 -0.80120946  0.58499143 -0.9493628   0.26586757 -0.78764014\n",
      " -0.14701094 -0.08257897 -0.30590452 -0.13888823 -0.71510712  0.57256934\n",
      "  0.06820257 -0.79907733  0.25594041  0.37094843  0.79823014  0.19109672\n",
      " -0.55641593 -0.2437502   0.26392278]\n",
      "Array of beta of TNC [ 0.09507134 -0.80121384  0.58498806 -0.94936914  0.2658734  -0.78764312\n",
      " -0.147024   -0.08258008 -0.30590415 -0.13888517 -0.71511221  0.57256367\n",
      "  0.06820415 -0.79907529  0.25594653  0.37095182  0.79823729  0.19110582\n",
      " -0.55641532 -0.24375744  0.26392906]\n",
      "Array of beta of Melder-Mead [ 0.26342056 -0.51476689  0.34795249 -0.76430795  0.12397376 -0.61059554\n",
      " -0.04940258 -0.22365583 -0.34970187  0.1335899  -0.616851    0.51804681\n",
      "  0.20077316 -0.361297    0.47040944  0.29253097  0.82673951 -0.11249968\n",
      " -0.07037682 -0.49368342  0.26305985]\n"
     ]
    }
   ],
   "source": [
    "print(\"Array of beta of SLSQP {}\".format(beta_s))\n",
    "print(\"Array of beta of TNC {}\".format(beta_t))\n",
    "print(\"Array of beta of Melder-Mead {}\".format(beta_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty arrays to store the values\n",
    "\n",
    "# Empty array of estimated respones obtained by SLSQP\n",
    "Y_s = np.empty(0)\n",
    "# Empyt array of estimated respones obtained by TNC\n",
    "Y_t = np.empty(0)\n",
    "# Empty array of estimated respones obtained by Nelder-Mead\n",
    "Y_m = np.empty(0)\n",
    "\n",
    "# Array of estimated responses obtained by SLSQP in the testing partition\n",
    "Y_s = np.append(Y_s,1/(1+np.exp(-np.dot(Xs,beta_s.T))))\n",
    "# Array of estimated responses obtained by TNC in the testing partition\n",
    "Y_t = np.append(Y_t,1/(1+np.exp(-np.dot(Xs,beta_t.T))))\n",
    "# Array of estimated responses obtained by Nelder Mead in the testing partition\n",
    "Y_m = np.append(Y_m,1/(1+np.exp(-np.dot(Xs,beta_m.T))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4 defines an observation classified incorrectly when one of the following conditions is met:\n",
    "* $Å·_{i} â¥$ $0.5$ $and$ $y_{i} = 0$\n",
    "* $Å·_{i} <$ $0.5$ $and$ $y_{i} = 1$\n",
    "\n",
    "In order to speed the computational time a bit, a unique condition will be applied in the cell below to detect whether an observation is classified incorrectly \n",
    "* $|Å·_{i}-y_{i}| â¥ 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of incorrectly classified observation with SLSQP is 0.205\n",
      "The proportion of incorrectly classified observation with TNC is 0.205\n",
      "The proportion of incorrectly classified observation with Melder-Mead is 0.25\n"
     ]
    }
   ],
   "source": [
    "Error_s = np.empty(0)\n",
    "Error_t = np.empty(0)\n",
    "Error_m = np.empty(0)\n",
    "\n",
    "# The observations classified incorrectly will be those which its predicted difference \n",
    "for i in range(0,len(Is)):\n",
    "    dif_s = abs(Ys[i]-Y_s[i])\n",
    "    if dif_s>=0.5:\n",
    "        Error_s = np.append(Error_s,1)\n",
    "    else:\n",
    "        Error_s = np.append(Error_s,0)\n",
    "    dif_t = abs(Ys[i]-Y_t[i])\n",
    "    if dif_t>=0.5:\n",
    "        Error_t = np.append(Error_t,1)\n",
    "    else:\n",
    "        Error_t = np.append(Error_t,0)    \n",
    "    dif_m = abs(Ys[i]-Y_m[i])\n",
    "    if dif_m>=0.5:\n",
    "        Error_m = np.append(Error_m,1)\n",
    "    else:\n",
    "        Error_m = np.append(Error_m,0)\n",
    "        \n",
    "print('The proportion of incorrectly classified observation with SLSQP is {}'.format((np.sum(Error_s))/len(Is)))\n",
    "print('The proportion of incorrectly classified observation with TNC is {}'.format((np.sum(Error_t))/len(Is)))\n",
    "print('The proportion of incorrectly classified observation with Melder-Mead is {}'.format((np.sum(Error_m))/len(Is)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Estimation using Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 5 the goal is to estimate $\\beta$ as the coefficients that maximize the logarithm of the likelihood function. \n",
    "The process to estimate $\\beta$ is analogous to that of section 3: first code the function, and then its gradient. Once that step is completed, *scipy.optimize* library is called to maximize the likelihood by finding $\\beta$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is now defined according to maximum likelihood criterion\n",
    "def max_li(beta, X, Y):\n",
    "    output = np.empty((0, p+1))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        exp = np.exp(-beta*X[i])\n",
    "        sumh1 = Y[i]*np.log(1/(1+exp))\n",
    "        sumh2 = (1-Y[i])*np.log(1-(1/(1+exp)))\n",
    "        # Change the sign of the expression to maximize when calling scipy.optimize.minimize\n",
    "        li = -(sumh1 + sumh2) \n",
    "        output = np.append(output,np.array([li]),axis=0)\n",
    "    output = np.sum(output)\n",
    "    return output\n",
    "\n",
    "# Initialize the values of beta to call minimize\n",
    "beta0 = np.zeros(p+1)\n",
    "\n",
    "# Define the gradient (jacobian) of the objective function\n",
    "def grad_max_li(beta, X, Y):\n",
    "    gg = np.empty((0,p+1))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        exp = np.exp(beta*X[i])\n",
    "        num = X[i]*((Y[i] - 1)*exp + Y[i])\n",
    "        den = exp + 1\n",
    "        comp = -(num/den)\n",
    "        gg = np.append(gg, np.array([comp]), axis=0)\n",
    "    gg = np.sum(gg, axis=0)\n",
    "    return gg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference approx:  -896.626975\n",
      "Directional derivative:    -896.636686\n"
     ]
    }
   ],
   "source": [
    "# Check that the derivative routine for the objective function is computing correct values\n",
    "\n",
    "## Initial values for  beta, u and delta\n",
    "\n",
    "beta_0 = np.ones(p+1)\n",
    "delta_v = 1.0e-4\n",
    "u = np.random.normal(0,1,size=p+1)\n",
    "u = u/np.linalg.norm(u,ord=2)\n",
    "\n",
    "## Values of the objective function at  alpha  and  alpha + delta*u\n",
    "\n",
    "beta_1 = beta_0 + delta_v*u\n",
    "\n",
    "v1 = max_li(beta_1,X,Y)\n",
    "v0 = max_li(beta_0,X,Y)\n",
    "\n",
    "## Compute the gradient at  alpha\n",
    "\n",
    "grad_0 = grad_max_li(beta_0,X,Y)\n",
    "\n",
    "## Compare the value of the directional derivative  g'u  with the \n",
    "## finite difference approximation  (f_1 - f_0)/delta\n",
    "\n",
    "fd_appr = (v1 - v0)/delta_v\n",
    "dir_der = np.dot(grad_0.T,u)\n",
    "print('Finite difference approx: %12.6f' %fd_appr)\n",
    "print('Directional derivative:   %12.6f' %dir_der)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values obtained with finite difference approx and directional derivative are very similar. Therefore the derivative routine to generate the gradient is considered to be consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider several methods to estimate $\\beta$ parameters in order to maximize the logarithm of the likelihood function. In this ocassion it is important to notice that the sign of the expression was changed before because *scipy.optimize* can only minimize. But now we actually want to maximize. The algorithms that will be applied are **CG (Conjugate Gradient), L-BFGS-B and Newton-CG\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vickytron/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n",
      "/home/vickytron/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 10560.997909\n",
      "         Iterations: 13\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Running time =  2.10673\n",
      "     fun: 10560.997909033733\n",
      "     jac: array([-2.06875007e-07,  1.03824439e-07,  5.94639371e-08, -5.33697334e-07,\n",
      "       -3.27510039e-07,  2.15609844e-06, -3.83442492e-06,  1.16486659e-06,\n",
      "        1.23289421e-06,  1.98678045e-06,  3.19783485e-07, -1.20219245e-07,\n",
      "       -1.99932091e-07, -5.00755130e-07, -5.88309885e-07,  1.38534001e-07,\n",
      "        1.04520030e-06, -1.27143438e-06, -6.38834809e-07,  1.97364565e-06,\n",
      "        1.39075116e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 28\n",
      "     nit: 13\n",
      "    njev: 28\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.09507154, -0.79068133,  0.61773011, -1.02793307,  0.27305402,\n",
      "       -0.87925146, -0.14546646, -0.08309907, -0.32637294, -0.14137104,\n",
      "       -0.7902052 ,  0.60798285,  0.06778329, -0.86772932,  0.26399682,\n",
      "        0.38683362,  0.89833767,  0.19300853, -0.56630958, -0.25668081,\n",
      "        0.26605792])\n",
      "\n",
      "Values of the beta coefficients obtained with CG:\n",
      "beta   0   0.095\n",
      "beta   1  -0.791\n",
      "beta   2   0.618\n",
      "beta   3  -1.028\n",
      "beta   4   0.273\n",
      "beta   5  -0.879\n",
      "beta   6  -0.145\n",
      "beta   7  -0.083\n",
      "beta   8  -0.326\n",
      "beta   9  -0.141\n",
      "beta  10  -0.790\n",
      "beta  11   0.608\n",
      "beta  12   0.068\n",
      "beta  13  -0.868\n",
      "beta  14   0.264\n",
      "beta  15   0.387\n",
      "beta  16   0.898\n",
      "beta  17   0.193\n",
      "beta  18  -0.566\n",
      "beta  19  -0.257\n",
      "beta  20   0.266\n"
     ]
    }
   ],
   "source": [
    "# First method CG\n",
    "\n",
    "time_start1 = time.process_time()\n",
    "result1 = minimize(max_li,beta0,args=(Xt,Yt),method='CG',jac=grad_max_li,options={'disp': True})\n",
    "time_elapsed1 = (time.process_time() - time_start1) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed1))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result1)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with CG:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result1.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time =  0.75499\n",
      "      fun: 10560.99791148206\n",
      " hess_inv: <21x21 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.00016595,  0.00066187,  0.00112846,  0.0004197 ,  0.00130656,\n",
      "        0.00140384,  0.00034056,  0.00655305, -0.03883184,  0.01215855,\n",
      "        0.00046083, -0.00180406, -0.00314061,  0.00119106, -0.00380459,\n",
      "       -0.01184473, -0.00088116, -0.00199299, -0.00017086,  0.00624321,\n",
      "       -0.00697784])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 10\n",
      "      nit: 9\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.09507071, -0.79067768,  0.61773498, -1.02793059,  0.2730579 ,\n",
      "       -0.87924504, -0.14546457, -0.08308186, -0.32646793, -0.1413398 ,\n",
      "       -0.7902031 ,  0.60797615,  0.06777479, -0.86772354,  0.26398721,\n",
      "        0.3868021 ,  0.89833394,  0.19300138, -0.56631041, -0.25666767,\n",
      "        0.26603849])\n",
      "\n",
      "Values of the beta coefficients obtained with L-BFGS-B:\n",
      "beta   0   0.095\n",
      "beta   1  -0.791\n",
      "beta   2   0.618\n",
      "beta   3  -1.028\n",
      "beta   4   0.273\n",
      "beta   5  -0.879\n",
      "beta   6  -0.145\n",
      "beta   7  -0.083\n",
      "beta   8  -0.326\n",
      "beta   9  -0.141\n",
      "beta  10  -0.790\n",
      "beta  11   0.608\n",
      "beta  12   0.068\n",
      "beta  13  -0.868\n",
      "beta  14   0.264\n",
      "beta  15   0.387\n",
      "beta  16   0.898\n",
      "beta  17   0.193\n",
      "beta  18  -0.566\n",
      "beta  19  -0.257\n",
      "beta  20   0.266\n"
     ]
    }
   ],
   "source": [
    "# Second method L-BFGS-B\n",
    "\n",
    "time_start2 = time.process_time()\n",
    "result2 = minimize(max_li,beta0,args=(Xt,Yt),method='L-BFGS-B',jac=grad_max_li,options={'disp': True})\n",
    "time_elapsed2 = (time.process_time() - time_start2) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed2))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result2)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with L-BFGS-B:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result2.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 10560.997909\n",
      "         Iterations: 11\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 48\n",
      "         Hessian evaluations: 0\n",
      "Running time =  2.04091\n",
      "     fun: 10560.997909033737\n",
      "     jac: array([ 1.51533027e-04, -6.61285730e-04,  1.18190385e-05,  6.40243346e-04,\n",
      "       -8.74492946e-06, -5.03770617e-05, -2.88020373e-04, -4.28070029e-08,\n",
      "       -6.11485340e-05, -3.03825530e-08, -4.98572241e-05, -1.06481557e-09,\n",
      "       -4.53670221e-07, -2.26946189e-04,  3.11741335e-06,  1.65943839e-07,\n",
      "        2.52555979e-06,  2.88347301e-07, -3.51397222e-04,  2.19854274e-05,\n",
      "       -6.47850193e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 12\n",
      "    nhev: 0\n",
      "     nit: 11\n",
      "    njev: 48\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.09507151, -0.79068132,  0.61773011, -1.02793296,  0.27305402,\n",
      "       -0.87925146, -0.14546642, -0.08309907, -0.326373  , -0.14137105,\n",
      "       -0.79020518,  0.60798285,  0.06778329, -0.86772925,  0.26399682,\n",
      "        0.38683362,  0.89833766,  0.19300854, -0.56630949, -0.25668078,\n",
      "        0.26605791])\n",
      "\n",
      "Values of the beta coefficients obtained with Newton CG:\n",
      "beta   0   0.095\n",
      "beta   1  -0.791\n",
      "beta   2   0.618\n",
      "beta   3  -1.028\n",
      "beta   4   0.273\n",
      "beta   5  -0.879\n",
      "beta   6  -0.145\n",
      "beta   7  -0.083\n",
      "beta   8  -0.326\n",
      "beta   9  -0.141\n",
      "beta  10  -0.790\n",
      "beta  11   0.608\n",
      "beta  12   0.068\n",
      "beta  13  -0.868\n",
      "beta  14   0.264\n",
      "beta  15   0.387\n",
      "beta  16   0.898\n",
      "beta  17   0.193\n",
      "beta  18  -0.566\n",
      "beta  19  -0.257\n",
      "beta  20   0.266\n"
     ]
    }
   ],
   "source": [
    "# Third method Newton CG\n",
    "\n",
    "time_start3 = time.process_time()\n",
    "result3 = minimize(max_li,beta0,args=(Xt,Yt),method='Newton-CG',jac=grad_max_li,options={'disp': True})\n",
    "time_elapsed3 = (time.process_time() - time_start3) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed3))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result3)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with Newton CG:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result3.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is table summarizing the results obtained for each of the three methods **CG, L-BFGS-B and Newton-CG**\n",
    "\n",
    "|Method |Running time (s)|Number of iterations|OF Evaluations|JAC Evaluations|OF Value|\n",
    "|---|---|---|---|---|---|\n",
    "|**CG** |\t2.10 |\t13 |\t28 |\t28 |\t10560.99|\n",
    "|**L-BFGS-B**\t|0.75\t|\t9 |\t10|\t-|\t10560.99|\n",
    "|**Newton-CG**\t|2.04\t|\t11|\t12|\t48|\t10560.99|\n",
    "\n",
    "**Note: the results depend on the randomness introduced when generaring the data, so the values will change if the code is executed again**\n",
    "\n",
    "The running time three of three algorithms as a group is significantly quicker than for the case of least squares criterion. The fastest one is L-BFGS-B, but the output offers less information than CG and Newton CG. One example of this is the number of evaluations of the jacobian, which is not shown with L-BFGS-B.\n",
    "\n",
    "The value of the objective function (OF) is almost the same for the three methods (just two decimals are displayed in the table). Furthermore, the value is quite high, which is something reasonable considering that the goal is to maximize the logarithm of the likelihood. \n",
    "\n",
    "Now regarding the values obtained for the $\\beta$ parameters, they are all very closed to zero. This is due to the fact that *scipy.optimize* library maximizes the logarithm of the likelihood (because the sign was changed in advanced). The graph below show, how small values of $Ï(Î²; x)$ make $-log(Ï(Î²; x))$ grow close to zero. Therefore, small values of $\\beta$ contribute to maximize $-log(Ï(Î²; x))$ and in extension, the logarithm of the likelihood.\n",
    "\n",
    "![alt text](Screen.png)\n",
    "\n",
    "An important consequence of dealing with values close to zero, is the overflow warning triggered by CG algorithm, because the $-log(Ï(Î²; x))$ term probably induces some numerical inestabilities in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Estimation with Regularization Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the objective function and the gradient are modified by adding a regularization term to their expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence the expression of the objective function and the gradient changes slightly by adding this regularization term. This term increases as the value of the parameters weights $(\\beta)$ increase; keep in mind that with regularization we add a new hyperparameter, to control the regularization strength.\n",
    "\n",
    "Regularization can be seen as a penalty against complexity. Increasing the regularization strength penalizes \"large\" weight coefficients. With the use of regularization our goal is to prevent that our model picks up \"peculiarities,\" \"noise,\" or \"pattern\" where there is none.\n",
    "\n",
    "More information can be consulted in the following link: https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is defined according to maximum likelihood criterion with regularization term\n",
    "def max_li2(beta, X, Y, rho):\n",
    "    output = np.empty((0, p+1))\n",
    "    nt = 0.8 * 1000\n",
    "    for i in range(0, X.shape[0]):\n",
    "        term = (rho/2)*((np.linalg.norm(beta,ord=2))**2)\n",
    "        exp = np.exp(-beta*X[i])\n",
    "        sumh1 = (Y[i]*np.log(1/(1+exp))) \n",
    "        sumh2 = (1-Y[i])*np.log(1-(1/(1+exp)))\n",
    "        # Change the sign of the expression to maximize when calling scipy.optimize.minimize\n",
    "        li = - (sumh1 + sumh2) \n",
    "        output = np.append(output,np.array([li]),axis=0)\n",
    "    # Finally add the regularization term because sign was changed\n",
    "    output = (1/nt)*np.sum(output)+term\n",
    "    return output\n",
    "\n",
    "# Initialize the values of beta to call minimize\n",
    "beta0 = np.zeros(p+1)\n",
    "\n",
    "# Define the gradient (jacobian) of the objective function\n",
    "def grad_max_li2(beta, X, Y,rho):\n",
    "    gg = np.empty((0,p+1))\n",
    "    nt = 0.8 * 1000\n",
    "    for i in range(0, X.shape[0]):\n",
    "        term = rho * beta\n",
    "        exp = np.exp(beta*X[i])\n",
    "        num = X[i]*((Y[i] - 1)*exp + Y[i])\n",
    "        den = exp + 1\n",
    "        comp = -(num/den)\n",
    "        gg = np.append(gg, np.array([comp]), axis=0)\n",
    "    gg = (1/nt)*np.sum(gg, axis=0)+term\n",
    "    return gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference approx:    -5.529979\n",
      "Directional derivative:      -5.530374\n"
     ]
    }
   ],
   "source": [
    "# Check that the derivative routine for the objective function is computing correct values\n",
    "\n",
    "## Initial values for  beta, u, delta and rho\n",
    "\n",
    "beta_0 = np.ones(p+1)\n",
    "delta_v = 1.0e-4\n",
    "u = np.random.normal(0,1,size=p+1)\n",
    "u = u/np.linalg.norm(u,ord=2)\n",
    "rho = np.random.uniform(low=0.1, high=10, size=None)\n",
    "\n",
    "## Values of the objective function at  beta  and  beta + delta*u\n",
    "\n",
    "beta_1 = beta_0 + delta_v*u\n",
    "\n",
    "v1 = max_li2(beta_1,Xt,Yt,rho)\n",
    "v0 = max_li2(beta_0,Xt,Yt,rho)\n",
    "\n",
    "## Compute the gradient at beta\n",
    "\n",
    "grad_0 = grad_max_li2(beta_0,Xt,Yt,rho)\n",
    "\n",
    "## Compare the value of the directional derivative  g'u  with the \n",
    "## finite difference approximation  (f_1 - f_0)/delta\n",
    "\n",
    "fd_appr = (v1 - v0)/delta_v\n",
    "dir_der = np.dot(grad_0.T,u)\n",
    "print('Finite difference approx: %12.6f' %fd_appr)\n",
    "print('Directional derivative:   %12.6f' %dir_der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values obtained with finite difference approx and directional derivative are very similar. Therefore the derivative routine to generate the gradient is considered to be consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider several methods to estimate $\\beta$ parameters in order to maximize the logarithm of the likelihood function. In this ocassion it is important to notice that the sign of the expression was changed before because *scipy.optimize* can only minimize. But now we actually want to maximize. The algorithms that will be applied are **CG (Conjugate Gradient), L-BFGS-B and Newton-CG\"** so the comparison with maximum likelihood is done using the same algorithms. There will be two values of $\\rho$ to run each of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.476145799571082\n",
      "6.914192121869568\n"
     ]
    }
   ],
   "source": [
    "rho1 = np.random.uniform(low=0.1, high=10, size=None)\n",
    "rho2 = np.random.uniform(low=0.1, high=10, size=None)\n",
    "print(rho1)\n",
    "print(rho2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 14.417046\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "Running time =  0.80967\n",
      "     fun: 14.417046130122307\n",
      "     jac: array([-2.78835626e-07, -1.19729139e-07,  3.93188913e-07,  5.83571012e-08,\n",
      "        2.04155819e-07,  2.81353722e-07,  5.10936322e-07, -6.25645344e-08,\n",
      "        1.27465312e-07, -9.17720556e-08,  4.24653963e-07, -3.91310168e-08,\n",
      "        5.32684191e-08, -1.25928833e-07,  1.32219480e-08, -1.54541306e-07,\n",
      "        2.81915741e-07, -6.34723916e-09,  2.34085152e-07,  2.05699272e-07,\n",
      "        1.60995463e-07])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 9\n",
      "     nit: 4\n",
      "    njev: 9\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.00414759, -0.04264865,  0.03942946, -0.09319869,  0.02088595,\n",
      "       -0.08777164, -0.00586625, -0.00669468, -0.03135859, -0.01178494,\n",
      "       -0.06365367,  0.04880707,  0.00529632, -0.0620767 ,  0.02391046,\n",
      "        0.03618221,  0.09116791,  0.01180958, -0.03006673, -0.02782679,\n",
      "        0.02169272])\n",
      "\n",
      "Values of the beta coefficients obtained with CG and rho1:\n",
      "beta   0   0.004\n",
      "beta   1  -0.043\n",
      "beta   2   0.039\n",
      "beta   3  -0.093\n",
      "beta   4   0.021\n",
      "beta   5  -0.088\n",
      "beta   6  -0.006\n",
      "beta   7  -0.007\n",
      "beta   8  -0.031\n",
      "beta   9  -0.012\n",
      "beta  10  -0.064\n",
      "beta  11   0.049\n",
      "beta  12   0.005\n",
      "beta  13  -0.062\n",
      "beta  14   0.024\n",
      "beta  15   0.036\n",
      "beta  16   0.091\n",
      "beta  17   0.012\n",
      "beta  18  -0.030\n",
      "beta  19  -0.028\n",
      "beta  20   0.022\n"
     ]
    }
   ],
   "source": [
    "# First method CG with rho1\n",
    "\n",
    "time_start1 = time.process_time()\n",
    "result1 = minimize(max_li2,beta0,args=(Xt,Yt,rho1),jac=grad_max_li2,method='CG',options={'disp': True})\n",
    "time_elapsed1 = (time.process_time() - time_start1) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed1))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result1)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with CG and rho1:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result1.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 14.442911\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "Running time =  0.80721\n",
      "     fun: 14.442911318518162\n",
      "     jac: array([-1.24535461e-07, -4.97199707e-08,  1.74193893e-07,  3.01627334e-08,\n",
      "        9.13405561e-08,  1.20885718e-07,  2.27722344e-07, -2.80716069e-08,\n",
      "        5.59645492e-08, -4.14828144e-08,  1.94169361e-07, -1.62225871e-08,\n",
      "        2.37861769e-08, -5.77964845e-08,  7.39383102e-09, -6.83015988e-08,\n",
      "        1.32710237e-07, -4.27791867e-09,  1.08017773e-07,  9.40265243e-08,\n",
      "        7.28237525e-08])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 9\n",
      "     nit: 4\n",
      "    njev: 9\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.00331508, -0.03423497,  0.03171217, -0.07628203,  0.01682019,\n",
      "       -0.07186107, -0.00468569, -0.00539302, -0.02538013, -0.00950059,\n",
      "       -0.05159682,  0.03945672,  0.00426422, -0.05013985,  0.01932124,\n",
      "        0.02928682,  0.07457148,  0.00947552, -0.02410648, -0.02258083,\n",
      "        0.01749038])\n",
      "\n",
      "Values of the beta coefficients obtained with CG and rho2:\n",
      "beta   0   0.003\n",
      "beta   1  -0.034\n",
      "beta   2   0.032\n",
      "beta   3  -0.076\n",
      "beta   4   0.017\n",
      "beta   5  -0.072\n",
      "beta   6  -0.005\n",
      "beta   7  -0.005\n",
      "beta   8  -0.025\n",
      "beta   9  -0.010\n",
      "beta  10  -0.052\n",
      "beta  11   0.039\n",
      "beta  12   0.004\n",
      "beta  13  -0.050\n",
      "beta  14   0.019\n",
      "beta  15   0.029\n",
      "beta  16   0.075\n",
      "beta  17   0.009\n",
      "beta  18  -0.024\n",
      "beta  19  -0.023\n",
      "beta  20   0.017\n"
     ]
    }
   ],
   "source": [
    "# First method CG with rho2\n",
    "\n",
    "time_start2 = time.process_time()\n",
    "result2 = minimize(max_li2,beta0,args=(Xt,Yt,rho2),method='CG',jac=grad_max_li2,options={'disp': True})\n",
    "time_elapsed2 = (time.process_time() - time_start2) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed2))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result2)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with CG and rho2:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result2.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time =  0.53633\n",
      "      fun: 14.417046130122317\n",
      " hess_inv: <21x21 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-1.66738051e-07, -1.18657518e-07,  2.06634744e-07,  2.25067814e-07,\n",
      "        8.62048977e-08,  6.73041860e-07,  3.15256864e-07, -2.49163776e-08,\n",
      "        2.10787960e-08, -3.03076858e-08, -9.71382527e-08, -3.97621399e-08,\n",
      "        2.35618265e-08,  1.03127810e-08, -1.35964584e-08, -2.19542035e-08,\n",
      "        7.33894281e-07,  1.35187777e-08,  8.19264298e-08, -1.29140194e-07,\n",
      "        5.07640237e-08])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 6\n",
      "      nit: 4\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.00414761, -0.04264865,  0.03942943, -0.09319866,  0.02088594,\n",
      "       -0.08777158, -0.00586629, -0.00669467, -0.03135861, -0.01178493,\n",
      "       -0.06365376,  0.04880707,  0.00529631, -0.06207668,  0.02391045,\n",
      "        0.03618223,  0.09116798,  0.01180958, -0.03006676, -0.02782684,\n",
      "        0.0216927 ])\n",
      "\n",
      "Values of the beta coefficients obtained with L-BFGS-B and rho1:\n",
      "beta   0   0.004\n",
      "beta   1  -0.043\n",
      "beta   2   0.039\n",
      "beta   3  -0.093\n",
      "beta   4   0.021\n",
      "beta   5  -0.088\n",
      "beta   6  -0.006\n",
      "beta   7  -0.007\n",
      "beta   8  -0.031\n",
      "beta   9  -0.012\n",
      "beta  10  -0.064\n",
      "beta  11   0.049\n",
      "beta  12   0.005\n",
      "beta  13  -0.062\n",
      "beta  14   0.024\n",
      "beta  15   0.036\n",
      "beta  16   0.091\n",
      "beta  17   0.012\n",
      "beta  18  -0.030\n",
      "beta  19  -0.028\n",
      "beta  20   0.022\n"
     ]
    }
   ],
   "source": [
    "# Second method L-BFGS-B with rho1\n",
    "\n",
    "time_start3 = time.process_time()\n",
    "result3 = minimize(max_li2,beta0,args=(Xt,Yt,rho1),method='L-BFGS-B',jac=grad_max_li2,options={'disp': True})\n",
    "time_elapsed3 = (time.process_time() - time_start3) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed3))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result3)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with L-BFGS-B and rho1:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result3.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time =  0.45790\n",
      "      fun: 14.44291131853947\n",
      " hess_inv: <21x21 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-3.41521040e-06,  6.88928682e-06, -3.39322479e-08, -3.53107818e-07,\n",
      "        1.37451168e-06, -2.66299867e-06,  5.65865620e-06, -5.10698802e-07,\n",
      "       -2.37631358e-06, -1.13986664e-06, -1.60701159e-06,  4.90526511e-06,\n",
      "        2.92386999e-07, -6.85736505e-06,  2.55662324e-06,  2.64555863e-06,\n",
      "       -5.22875415e-06, -2.49351954e-06,  9.05164802e-06,  4.24934303e-07,\n",
      "        2.16264783e-06])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 5\n",
      "      nit: 3\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.00331462, -0.03423402,  0.03171214, -0.07628208,  0.01682036,\n",
      "       -0.07186142, -0.00468493, -0.00539309, -0.02538045, -0.00950073,\n",
      "       -0.05159706,  0.03945738,  0.00426426, -0.05014076,  0.01932158,\n",
      "        0.02928718,  0.0745708 ,  0.00947518, -0.02410524, -0.02258078,\n",
      "        0.01749067])\n",
      "\n",
      "Values of the beta coefficients obtained with L-BFGS-B and rho2:\n",
      "beta   0   0.003\n",
      "beta   1  -0.034\n",
      "beta   2   0.032\n",
      "beta   3  -0.076\n",
      "beta   4   0.017\n",
      "beta   5  -0.072\n",
      "beta   6  -0.005\n",
      "beta   7  -0.005\n",
      "beta   8  -0.025\n",
      "beta   9  -0.010\n",
      "beta  10  -0.052\n",
      "beta  11   0.039\n",
      "beta  12   0.004\n",
      "beta  13  -0.050\n",
      "beta  14   0.019\n",
      "beta  15   0.029\n",
      "beta  16   0.075\n",
      "beta  17   0.009\n",
      "beta  18  -0.024\n",
      "beta  19  -0.023\n",
      "beta  20   0.017\n"
     ]
    }
   ],
   "source": [
    "# Second method L-BFGS-B with rho2\n",
    "\n",
    "time_start4 = time.process_time()\n",
    "result4 = minimize(max_li2,beta0,args=(Xt,Yt,rho2),method='L-BFGS-B',jac=grad_max_li2,options={'disp': True})\n",
    "time_elapsed4 = (time.process_time() - time_start4) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed4))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result4)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with L-BFGS-B and rho2:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result4.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 14.417046\n",
      "         Iterations: 4\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 18\n",
      "         Hessian evaluations: 0\n",
      "Running time =  0.88503\n",
      "     fun: 14.41704613012222\n",
      "     jac: array([-1.16971635e-05,  5.00785928e-05, -2.74360522e-05, -4.13651563e-05,\n",
      "       -9.45841693e-06, -4.40423544e-05,  1.84673883e-05,  2.72828341e-06,\n",
      "        5.90669828e-07,  3.59425680e-06, -2.80832802e-07, -3.08263827e-06,\n",
      "       -2.62218091e-06,  7.26046367e-06, -2.09039799e-06, -5.95318767e-07,\n",
      "        2.96241430e-05, -1.54471302e-05,  4.64909235e-05, -1.31994621e-07,\n",
      "       -6.22858085e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 5\n",
      "    nhev: 0\n",
      "     nit: 4\n",
      "    njev: 18\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.00414765, -0.04264864,  0.0394294 , -0.0931987 ,  0.02088592,\n",
      "       -0.08777169, -0.00586633, -0.00669467, -0.03135861, -0.01178492,\n",
      "       -0.06365374,  0.04880708,  0.00529631, -0.06207668,  0.02391046,\n",
      "        0.03618224,  0.09116787,  0.01180958, -0.03006677, -0.02782682,\n",
      "        0.02169269])\n",
      "\n",
      "Values of the beta coefficients obtained with Newton CG and rho1:\n",
      "beta   0   0.004\n",
      "beta   1  -0.043\n",
      "beta   2   0.039\n",
      "beta   3  -0.093\n",
      "beta   4   0.021\n",
      "beta   5  -0.088\n",
      "beta   6  -0.006\n",
      "beta   7  -0.007\n",
      "beta   8  -0.031\n",
      "beta   9  -0.012\n",
      "beta  10  -0.064\n",
      "beta  11   0.049\n",
      "beta  12   0.005\n",
      "beta  13  -0.062\n",
      "beta  14   0.024\n",
      "beta  15   0.036\n",
      "beta  16   0.091\n",
      "beta  17   0.012\n",
      "beta  18  -0.030\n",
      "beta  19  -0.028\n",
      "beta  20   0.022\n"
     ]
    }
   ],
   "source": [
    "# Third method Newton CG and rho1\n",
    "\n",
    "time_start5 = time.process_time()\n",
    "result5 = minimize(max_li2,beta0,args=(Xt,Yt,rho1),method='Newton-CG',jac=grad_max_li2,options={'disp': True})\n",
    "time_elapsed5 = (time.process_time() - time_start5) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed5))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result5)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with Newton CG and rho1:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result5.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 14.442911\n",
      "         Iterations: 4\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 18\n",
      "         Hessian evaluations: 0\n",
      "Running time =  0.88591\n",
      "     fun: 14.44291131851815\n",
      "     jac: array([-6.41677471e-06,  2.77071815e-05, -1.52917380e-05, -2.32331354e-05,\n",
      "       -5.32410750e-06, -2.45277709e-05,  1.01202495e-05,  1.54091846e-06,\n",
      "        3.97953913e-07,  2.04386706e-06, -1.01163121e-07, -1.83378052e-06,\n",
      "       -1.47479267e-06,  4.19182282e-06, -1.24042076e-06, -4.05598342e-07,\n",
      "        1.63947903e-05, -8.54704302e-06,  2.56543877e-05, -6.62425505e-08,\n",
      "       -3.54290169e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 5\n",
      "    nhev: 0\n",
      "     nit: 4\n",
      "    njev: 18\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.0033151 , -0.03423497,  0.03171215, -0.07628203,  0.01682017,\n",
      "       -0.07186109, -0.00468572, -0.00539302, -0.02538013, -0.00950058,\n",
      "       -0.05159685,  0.03945672,  0.00426422, -0.05013985,  0.01932124,\n",
      "        0.02928683,  0.07457146,  0.00947552, -0.02410649, -0.02258084,\n",
      "        0.01749037])\n",
      "\n",
      "Values of the beta coefficients obtained with Newton CG and rho2:\n",
      "beta   0   0.003\n",
      "beta   1  -0.034\n",
      "beta   2   0.032\n",
      "beta   3  -0.076\n",
      "beta   4   0.017\n",
      "beta   5  -0.072\n",
      "beta   6  -0.005\n",
      "beta   7  -0.005\n",
      "beta   8  -0.025\n",
      "beta   9  -0.010\n",
      "beta  10  -0.052\n",
      "beta  11   0.039\n",
      "beta  12   0.004\n",
      "beta  13  -0.050\n",
      "beta  14   0.019\n",
      "beta  15   0.029\n",
      "beta  16   0.075\n",
      "beta  17   0.009\n",
      "beta  18  -0.024\n",
      "beta  19  -0.023\n",
      "beta  20   0.017\n"
     ]
    }
   ],
   "source": [
    "# Third method Newton CG and rho2\n",
    "\n",
    "time_start6 = time.process_time()\n",
    "result6 = minimize(max_li2,beta0,args=(Xt,Yt,rho2),method='Newton-CG',jac=grad_max_li2,options={'disp': True})\n",
    "time_elapsed6 = (time.process_time() - time_start6) \n",
    "\n",
    "#Print the running time\n",
    "\n",
    "print('Running time = %8.5f' %(time_elapsed6))\n",
    "\n",
    "# Print the output \n",
    "\n",
    "print(result6)\n",
    "\n",
    "## Print the estimated beta coefficients\n",
    "\n",
    "print('\\nValues of the beta coefficients obtained with Newton CG and rho2:')\n",
    "for i in range(p+1):\n",
    "   print('beta %3d %7.3f' %(i,result6.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is table summarizing the results obtained for each of the three methods **CG, L-BFGS-B and Newton-CG** comparing the influence of the regularization term\n",
    "\n",
    "|Method |Running time (s)|OF Value|\n",
    "|---|---|---|\n",
    "|**CG** |\t2.10 |\t10560.99|\n",
    "|**CG with Rho1** |\t0.80 |\t14.41 |\n",
    "|**CG with Rho2** |\t0.80 |\t14.44 |\n",
    "|**L-BFGS-B**\t|0.75\t|\t10560.99|\n",
    "|**L-BFGS-B with Rho1**\t|0.53\t|\t14.41 |\n",
    "|**L-BFGS-B with Rho2**\t|0.45\t|\t14.44 |\n",
    "|**Newton-CG**\t|2.03\t|\t10560.99|\n",
    "|**Newton-CG with Rho1**\t|0.88\t|\t14.41|\n",
    "|**Newton-CG with Rho1**\t|0.88\t|\t14.44|\n",
    "\n",
    "**Note: the results depend on the randomness introduced when generaring the data, so the values will change if the code is executed again**\n",
    "\n",
    "The effect of the regularization term is clear as the values of $\\beta$ parameters are very small. In fact, around 10 times smaller than without regularization term.\n",
    "\n",
    "The values of the OF with regularization term are also smaller than without it. This is reasonable as the regularization term includes a multiplication of $1/800$ in the output which contributes to the reduction.\n",
    "\n",
    "Finally, the running time of the algorithms with regularization term is significantly faster than without regularization term.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
